<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Our usage of Per-Pixel Shading (PPS) information as a part of both novel loss functions and a novel refinement module enables state-of-the-art monocular depth estimation results on colonoscopy video data.">
  <meta name="keywords" content="Endoscopy, Monocular Depth Estimation, Photometric Refinement, Sim2Real transfer learning">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Leveraging Near-Field Lighting for Monocular Depth Estimation from Endoscopy Videos
  </title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <!-- <link rel="icon" href="./static/images/favicon.svg"> -->

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Leveraging Near-Field Lighting for Monocular Depth Estimation from Endoscopy Videos</h1>
          <h2 class="is-size-4 conference-title">arXiv</h2>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://www.cs.unc.edu/~akshaypa/">Akshay Paruchuri</a><sup>*</sup>,</span>
              <span class="author-block">
                Samuel Ehrenstein,
              </span>
              <span class="author-block">
                Shuxian Wang,
              </span>
              <span class="author-block">
                Inbar Fried,
              </span><br>
              <span class="author-block">
                <a href="https://www.cs.unc.edu/~smp/">Stephen M. Pizer</a>,
              </span>
              <span class="author-block">
                <a href="https://biag.cs.unc.edu/author/marc-niethammer/">Marc Niethammer</a>,
              </span>
            <span class="author-block">
              <a href="https://www.cs.unc.edu/~ronisen/">Roni Sengupta</a><sup>*</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">Department of Computer Science</span><br>
            <span class="author-block">University of North Carolina at Chapel Hill</span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>*</sup>Corresponding authors: {akshay, ronisen}@cs.unc.edu</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2403.17915"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/Roni-Lab/PPSNet"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video id="teaser" preload='auto' autoplay muted loop playsinline height="100%">
        <source src="./static/videos/ubfc-rppg_aug_examples_V5.mp4"
                type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        Examples of motion augmentation as applied to the UBFC-rPPG dataset. The driving videos utilized are from the TalkingHead-1KH dataset.
      </h2>

      <center><img src="./static/images/Teaser_Figure.png" 
      width="400" 
      height="300" /></center>
      <h2 class="subtitle has-text-centered">
        Our neural motion augmentation pipeline for the task of remote PPG estimation reduces error in heart rate estimation by up to 79% in inter-dataset results using TS-CAN and 47% over existing results using SOTA methods on PURE.
      </h2>
    </div>
  </div>
</section> -->

<!-- <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="./static/videos/teaser.mp4"
                type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        <span class="dnerf">Nerfies</span> turns selfie videos from your phone into
        free-viewpoint
        portraits.
      </h2>
    </div>
  </div>
</section>


<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-steve">
          <video poster="" id="steve" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/steve.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-chair-tp">
          <video poster="" id="chair-tp" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/chair-tp.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-shiba">
          <video poster="" id="shiba" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/shiba.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-fullbody">
          <video poster="" id="fullbody" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/fullbody.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-blueshirt">
          <video poster="" id="blueshirt" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/blueshirt.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-mask">
          <video poster="" id="mask" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/mask.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-coffee">
          <video poster="" id="coffee" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/coffee.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-toby">
          <video poster="" id="toby" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/toby2.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section> -->

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <center><img src="./static/images/teaser_figure.png"/></center>
      <h2 class="subtitle has-text-centered">
        We model near-field lighting, emitted by the endoscope and reflected
        by the surface, as Per-Pixel Shading (PPS). We use PPS features to perform depth
        refinement (PPSNet) on clinical data using teacher-student transfer learning and a
        PPS-informed self-supervision.
      </h2>
    </div>
  </div>
</section>

<section class="hero is-small" style="margin-top: 30px; padding-top: 0px;">
  <h2 class="title is-3" style="text-align: center;">Clinical Mesh Results</h2>
  <div class="hero-body">
        <div class="container">
          <div id="results-carousel" class="carousel results-carousel">
            <div class="item item-steve">
              <img src="./static/compare_depth/clinical_1.png" alt="Steve GIF" width="100%">
            </div>

            <div class="item item-steve">
              <img src="./static/compare_depth/clinical_2.png" alt="Steve GIF" width="100%">
            </div>
    
            <div class="item item-steve">
              <img src="./static/compare_depth/clinical_3.png" alt="Steve GIF" width="100%">
            </div>

            <div class="item item-steve">
              <img src="./static/compare_depth/clinical_4.png" alt="Steve GIF" width="100%">
            </div>

            <div class="item item-steve">
              <img src="./static/compare_depth/clinical_5.png" alt="Steve GIF" width="100%">
            </div>

            <div class="item item-steve">
              <img src="./static/compare_depth/clinical_6.png" alt="Steve GIF" width="100%">
            </div>

            <div class="item item-steve">
              <img src="./static/compare_depth/clinical_7.png" alt="Steve GIF" width="100%">
            </div>

            <div class="item item-steve">
              <img src="./static/compare_depth/clinical_8.png" alt="Steve GIF" width="100%">
            </div>

            </div>
            <div class="container is-max-desktop">
              <div class="columns is-centered">
                <div class="column is-full-width">
                </div>
              </div>
              </div>
        </div>
      </div>
    </div>
</section>

<section class="hero is-small" style="margin-top: 30px; padding-top: 0px;">
  <h2 class="title is-3" style="text-align: center;">Clinical Mesh Results</h2>
  <div class="hero-body">
        <div class="container">
          <div id="results-carousel" class="carousel results-carousel">
            <div class="item item-steve">
              <img src="./static/compare_mesh/clinical_1.png" alt="Steve GIF" width="100%">
            </div>

            <div class="item item-steve">
              <img src="./static/compare_mesh/clinical_2.png" alt="Steve GIF" width="100%">
            </div>
    
            <div class="item item-steve">
              <img src="./static/compare_mesh/clinical_3.png" alt="Steve GIF" width="100%">
            </div>

            <div class="item item-steve">
              <img src="./static/compare_mesh/clinical_4.png" alt="Steve GIF" width="100%">
            </div>

            </div>
            <div class="container is-max-desktop">
              <div class="columns is-centered">
                <div class="column is-full-width">
                </div>
              </div>
              </div>
        </div>
      </div>
    </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">How does it work?</h2>
        <h2 class="title is-4">Per-Pixel Shading (PPS) Representation</h2>
        <center><img src="./static/images/PPS_representation_figure.png"/></center>
        <div class="content has-text-justified">
          <p>
            Using depths and surface normals, we compute our proposed PPS representation. Our idea relies on the fact that the PPS is strongly correlated with the image intensity field except in regions of strong specularity and it ignores inter-reflections by only modeling direct, in-view illumination from
            the surface to the camera. We also observe that the usage of PPS is uniform and dependable across entire datasets such as <a href="https://durrlab.github.io/C3VD/?">C3VD</a>. As a result, we can utilize PPS in both supervised and self-supervised loss function variants.
          </p>
        </div>
        <h2 class="title is-4">Depth Refinement</h2>
        <div class="content has-text-justified">
          <p>
            Additionally, our approach involves making an initial depth prediction and then refining that depth prediction with the help of both RGB features and PPS features. A full forward pass of our approach is included in the below algorithm table.
          </p>
        </div>
        <center><img src="./static/images/Depth_refinement_table.png"/></center>
        <h2 class="title is-4">Training Protocol</h2>
        <div class="content has-text-justified">
          <p>
            Finally, in order to leverage both synthetic, phantom colonoscopy data and more challenging, real-world clinical data as a part of our training protocol, our approach involves training a student model on both synthetic data (e.g., C3VD) and clinical data with the guidance of a teacher model trained only on synthetic data.
          </p>
        </div>
        <center><img src="./static/images/teacher_student_figure.png"/></center>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Comparison to other methods</h2>
        <div class="content has-text-justified">
          <p>
            We compare our approach with existing monocular depth estimation techniques developed for endoscopy videos by testing on the synthetic <a href="https://durrlab.github.io/C3VD/?">C3VD</a> dataset. Ours-Student trains using both <a href="https://durrlab.github.io/C3VD/?">C3VD</a> and real data. Best results are shown in bold. Second best results are underlined.
          </p>
        </div>
        <center><img src="./static/images/main_result_table.png"/></center>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Additional Materials</h2>
        <div class="content has-text-justified">
          <p>
            In addition to our code release which currently includes <a href="https://drive.google.com/drive/folders/17778hK9_Zk9lSrDr5EPQnmLEUwLnpblB?usp=sharing">our pre-trained models</a> and <a href="https://drive.google.com/drive/folders/1QfacGUjaD1-ByC1XvukUzu84HGdwKXhF?usp=sharing">a preprocessed version of our C3VD test split</a>, we also release <a href="https://drive.google.com/drive/folders/1TnoHUtSYxRMW4J7uTwX8C3Jwzcm2cdgL?usp=sharing">mesh examples shown in the paper</a> and <a href="https://drive.google.com/drive/folders/1A86ZOU47_hgk1AVY1Fv1Vs-1Mx4M89GW?usp=sharing">our clinical data splits</a>. The clinical dataset itself, which includes oblique and en face views, will be fully released in the near future. Please refer to <a href="https://arxiv.org/abs/2403.17915">our full paper</a> for more details, including our ablations.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>Coming soon!</code></pre>
  </div>
</section>

<!-- <section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@misc{paruchuri2023motion,
      title={Motion Matters: Neural Motion Transfer for Better Camera Physiological Sensing}, 
      author={Akshay Paruchuri and Xin Liu and Yulu Pan and Shwetak Patel and Daniel McDuff and Soumyadip Sengupta},
      year={2023},
      eprint={2303.12059},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}</code></pre>
  </div>
</section> -->

<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This websites utilizes source code from <a
              href="https://github.com/nerfies/nerfies.github.io">here</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
